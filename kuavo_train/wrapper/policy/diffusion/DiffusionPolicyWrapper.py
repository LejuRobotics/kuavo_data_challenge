from lerobot.policies.diffusion.modeling_diffusion import DiffusionPolicy
from kuavo_train.utils.augmenter import (crop_image,
                                        resize_image)
from torch import Tensor, nn
import torch
from collections import deque
from lerobot.utils.constants import ACTION, OBS_ENV_STATE, OBS_IMAGES, OBS_STATE
from kuavo_train.wrapper.policy.diffusion.DiffusionConfigWrapper import CustomDiffusionConfigWrapper
from lerobot.policies.utils import (
    get_device_from_parameters,
    get_dtype_from_parameters,
    get_output_shape,
    populate_queues,
)

from kuavo_train.wrapper.policy.diffusion.DiffusionModelWrapper import CustomDiffusionModelWrapper
import os, builtins, threading, time
from pathlib import Path
from typing import TypeVar
from huggingface_hub import HfApi, ModelCard, ModelCardData, hf_hub_download
from huggingface_hub.constants import SAFETENSORS_SINGLE_FILE
from huggingface_hub.errors import HfHubHTTPError
import torchvision
import torchvision.transforms.functional

T = TypeVar("T", bound="CustomDiffusionPolicyWrapper")
OBS_DEPTH = "observation.depth"

class CustomDiffusionPolicyWrapper(DiffusionPolicy):
    def __init__(self,
                 config: CustomDiffusionConfigWrapper,
    ):
        vision_backbone = config.vision_backbone
        config.vision_backbone = "resnet18"  # Change vision backbone to ResNet18
        # this is important to call the parent constructor to setup normalization and queues,
        # to prevent original config not supported other vision backbone
        # super().__init__(config, dataset_stats)
        noise_scheduler = config.noise_scheduler_type
        config.noise_scheduler_type = "DDPM"
        super().__init__(config)
        config.vision_backbone = vision_backbone  # change back to the original config
        config.noise_scheduler_type = noise_scheduler
        
        self.diffusion = CustomDiffusionModelWrapper(config)


        # self._predicting = False
        # self._next_actions = None
        # self._lock = threading.Lock()

    def reset(self):
        """Clear observation and action queues. Should be called on `env.reset()`"""
        self._queues = {
            "observation.state": deque(maxlen=self.config.n_obs_steps),
            "action": deque(maxlen=self.config.n_action_steps),
        }
        if self.config.image_features:
            self._queues["observation.images"] = deque(maxlen=self.config.n_obs_steps)
        if self.config.use_depth and self.config.depth_features:
            self._queues["observation.depth"] = deque(maxlen=self.config.n_obs_steps)
        if self.config.env_state_feature:
            self._queues["observation.environment_state"] = deque(maxlen=self.config.n_obs_steps)
    
    @torch.no_grad()
    def select_action(self, batch: dict[str, Tensor],noise: Tensor | None = None, episode=0,step=0) -> Tensor:
        """Select a single action given environment observations.

        This method handles caching a history of observations and an action trajectory generated by the
        underlying diffusion model. Here's how it works:
          - `n_obs_steps` steps worth of observations are cached (for the first steps, the observation is
            copied `n_obs_steps` times to fill the cache).
          - The diffusion model generates `horizon` steps worth of actions.
          - `n_action_steps` worth of actions are actually kept for execution, starting from the current step.
        Schematically this looks like:
            ----------------------------------------------------------------------------------------------
            (legend: o = n_obs_steps, h = horizon, a = n_action_steps)
            |timestep            | n-o+1 | n-o+2 | ..... | n     | ..... | n+a-1 | n+a   | ..... | n-o+h |
            |observation is used | YES   | YES   | YES   | YES   | NO    | NO    | NO    | NO    | NO    |
            |action is generated | YES   | YES   | YES   | YES   | YES   | YES   | YES   | YES   | YES   |
            |action is used      | NO    | NO    | NO    | YES   | YES   | YES   | NO    | NO    | NO    |
            ----------------------------------------------------------------------------------------------
        Note that this means we require: `n_action_steps <= horizon - n_obs_steps + 1`. Also, note that
        "horizon" may not the best name to describe what the variable actually means, because this period is
        actually measured from the first observation which (if `n_obs_steps` > 1) happened in the past.
        """
        # 与环境交互时无需图像增强
        if ACTION in batch:
            batch.pop(ACTION)
        
        # batch = self.normalize_inputs(batch)
        
        random_crop = self.config.crop_is_random and self.training
        crop_position_list = []
        if self.config.image_features:
            batch = dict(batch)  # shallow copy so that adding a key doesn't modify the original
            for key in self.config.image_features:
                batch[key], crop_position = crop_image(batch[key],target_range=self.config.crop_shape,random_crop=random_crop)
                crop_position_list.append(crop_position)
                batch[key] = resize_image(batch[key],target_size=self.config.resize_shape, image_type="rgb")
            batch[OBS_IMAGES] = torch.stack([batch[key] for key in self.config.image_features], dim=-4)
        if self.config.use_depth and self.config.depth_features:
            batch = dict(batch)  # shallow copy so that adding a key doesn't modify the original

            for key, crop_position in zip(self.config.depth_features, crop_position_list):
                if len(crop_position) == 4:
                    batch[key] = torchvision.transforms.functional.crop(batch[key],*crop_position)
                else:
                    batch[key] = torchvision.transforms.functional.center_crop(batch[key],crop_position)
                batch[key] = resize_image(batch[key],target_size=self.config.resize_shape, image_type="depth")
            batch[OBS_DEPTH] = torch.stack([batch[key] for key in self.config.depth_features], dim=-4)
            batch[OBS_DEPTH] = batch[OBS_DEPTH].mean(dim=-3, keepdim=True)  # if multiple channels depth images, average them


        # NOTE: It's important that this happens after stacking the images into a single key.
        self._queues = populate_queues(self._queues, batch)

        if len(self._queues[ACTION]) == 0:
            # print(f"Episode {episode}, Step {step}: ~~~~~~~~~~~~~~Generating new action chunk using diffusion model...~~~~~~~~~~~~~~~~~~")
            actions = self.predict_action_chunk(batch, noise=noise)
            self._queues[ACTION].extend(actions.transpose(0, 1))

        action = self._queues[ACTION].popleft()
        return action
    

    # def _async_predict(self, batch, noise):
    #     """在后台线程中执行推理"""
    #     def worker():
    #         # 先设置标记，表示推理正在进行
    #         with self._lock:
    #             self._predicting = True

    #         # 执行耗时预测（不要在锁里面）
    #         actions = self.predict_action_chunk(batch, noise=noise)  # 耗时 0.26s

    #         # 将结果写入共享变量，同时更新状态
    #         with self._lock:
    #             self._next_actions = actions.transpose(0, 1)
    #             self._predicting = False

    #     threading.Thread(target=worker, daemon=True).start()


    # @torch.no_grad()
    # def select_action_old(self, batch: dict[str, Tensor],noise: Tensor | None = None, episode=0,step=0) -> Tensor:
    #     """Select a single action given environment observations.

    #     This method handles caching a history of observations and an action trajectory generated by the
    #     underlying diffusion model. Here's how it works:
    #       - `n_obs_steps` steps worth of observations are cached (for the first steps, the observation is
    #         copied `n_obs_steps` times to fill the cache).
    #       - The diffusion model generates `horizon` steps worth of actions.
    #       - `n_action_steps` worth of actions are actually kept for execution, starting from the current step.
    #     Schematically this looks like:
    #         ----------------------------------------------------------------------------------------------
    #         (legend: o = n_obs_steps, h = horizon, a = n_action_steps)
    #         |timestep            | n-o+1 | n-o+2 | ..... | n     | ..... | n+a-1 | n+a   | ..... | n-o+h |
    #         |observation is used | YES   | YES   | YES   | YES   | NO    | NO    | NO    | NO    | NO    |
    #         |action is generated | YES   | YES   | YES   | YES   | YES   | YES   | YES   | YES   | YES   |
    #         |action is used      | NO    | NO    | NO    | YES   | YES   | YES   | NO    | NO    | NO    |
    #         ----------------------------------------------------------------------------------------------
    #     Note that this means we require: `n_action_steps <= horizon - n_obs_steps + 1`. Also, note that
    #     "horizon" may not the best name to describe what the variable actually means, because this period is
    #     actually measured from the first observation which (if `n_obs_steps` > 1) happened in the past.
    #     """
    #     # 与环境交互时无需图像增强
    #     if ACTION in batch:
    #         batch.pop(ACTION)
        
    #     # batch = self.normalize_inputs(batch)
        
    #     random_crop = self.config.crop_is_random and self.training
    #     crop_position_list = []
    #     if self.config.image_features:
    #         batch = dict(batch)  # shallow copy so that adding a key doesn't modify the original
    #         for key in self.config.image_features:
    #             batch[key], crop_position = crop_image(batch[key],target_range=self.config.crop_shape,random_crop=random_crop)
    #             crop_position_list.append(crop_position)
    #             batch[key] = resize_image(batch[key],target_size=self.config.resize_shape, image_type="rgb")
    #         batch[OBS_IMAGES] = torch.stack([batch[key] for key in self.config.image_features], dim=-4)
    #     if self.config.use_depth and self.config.depth_features:
    #         batch = dict(batch)  # shallow copy so that adding a key doesn't modify the original

    #         for key, crop_position in zip(self.config.depth_features, crop_position_list):
    #             if len(crop_position) == 4:
    #                 batch[key] = torchvision.transforms.functional.crop(batch[key],*crop_position)
    #             else:
    #                 batch[key] = torchvision.transforms.functional.center_crop(batch[key],crop_position)
    #             batch[key] = resize_image(batch[key],target_size=self.config.resize_shape, image_type="depth")
    #         batch[OBS_DEPTH] = torch.stack([batch[key] for key in self.config.depth_features], dim=-4)
    #         batch[OBS_DEPTH] = batch[OBS_DEPTH].mean(dim=-3, keepdim=True)  # if multiple channels depth images, average them


    #     # NOTE: It's important that this happens after stacking the images into a single key.
    #     self._queues = populate_queues(self._queues, batch)

    #     if len(self._queues[ACTION]) <= (self.config.n_action_steps//2) and not self._predicting and self._next_actions is None:
    #         self._async_predict(batch, noise)
    #         print(f"Episode {episode}, Step {step}: ~~~~~~~~~~~~~~Generating new action chunk using diffusion model...~~~~~~~~~~~~~~~~~~")

    #     # (2) 如果真的没有动作可用，说明异步任务太慢 -> 等待它完成
    #     if len(self._queues[ACTION]) == 0:
    #         if self._predicting:
    #             print("⚠️ Waiting for model inference to complete...")
    #             while self._predicting:
    #                 time.sleep(0.1)
    #         # 推理结束后合并结果
    #         if self._next_actions is not None:
    #             with self._lock:
    #                 self._queues[ACTION].extend(self._next_actions)
    #                 self._next_actions = None

    #     # (3) 如果异步推理完成，队列还没空，提前合并结果
    #     if self._next_actions is not None and len(self._queues[ACTION]) > (self.config.n_action_steps//2):
    #         with self._lock:
    #             self._queues[ACTION].extend(self._next_actions)
    #             self._next_actions = None

    #     # (4) 正常取动作
    #     action = self._queues[ACTION].popleft()
    #     return action

    def forward(self, batch: dict[str, Tensor]) -> tuple[Tensor, None]:
        """Run the batch through the model and compute the loss for training or validation."""

        
        random_crop = self.config.crop_is_random and self.training
        crop_position = None
        if self.config.image_features:
            batch = dict(batch)  # shallow copy so that adding a key doesn't modify the original
            for key in self.config.image_features:
                batch[key], crop_position = crop_image(batch[key],target_range=self.config.crop_shape,random_crop=random_crop)
                batch[key] = resize_image(batch[key],target_size=self.config.resize_shape, image_type="rgb")
            # batch[OBS_IMAGES] = torch.stack([batch[key] for key in self.config.image_features], dim=-4)
        if self.config.use_depth and self.config.depth_features:
            batch = dict(batch)  # shallow copy so that adding a key doesn't modify the original

            for key in self.config.depth_features:
                if len(crop_position) == 4:
                    batch[key] = torchvision.transforms.functional.crop(batch[key],*crop_position)
                else:
                    batch[key] = torchvision.transforms.functional.center_crop(batch[key],crop_position)
                # print(batch[key].dtype,"~~~~~~~~~~~~~~~~~~")
                batch[key] = resize_image(batch[key],target_size=self.config.resize_shape, image_type="depth")
            # batch[OBS_DEPTH] = torch.stack([batch[key] for key in self.config.depth_features], dim=-4)


        # batch = self.normalize_inputs(batch)
        # batch[OBS_DEPTH] = torch.tensor(batch[OBS_DEPTH],dtype=batch[OBS_IMAGES].dtype)
        if self.config.image_features:
            batch = dict(batch)  # shallow copy so that adding a key doesn't modify the original
            batch[OBS_IMAGES] = torch.stack([batch[key] for key in self.config.image_features], dim=-4)
        if self.config.use_depth and self.config.depth_features:
            batch = dict(batch)  # shallow copy so that adding a key doesn't modify the original
            batch[OBS_DEPTH] = torch.stack([batch[key] for key in self.config.depth_features], dim=-4)
            batch[OBS_DEPTH] = batch[OBS_DEPTH].mean(dim=-3, keepdim=True)  # if multiple channels depth images, average them
            # print("mean depth:",batch[OBS_DEPTH].mean().item(),"max depth:",batch[OBS_DEPTH].max().item(),"min depth:",batch[OBS_DEPTH].min().item())
        # batch = self.normalize_targets(batch)
        # print(batch[OBS_DEPTH].shape, batch[OBS_DEPTH].max(), batch[OBS_DEPTH].min())
        # print(batch[OBS_IMAGES].shape, batch[OBS_IMAGES].max(), batch[OBS_IMAGES].min())
        # raise ValueError()

        loss = self.diffusion.compute_loss(batch)
        # no output_dict so returning None
        return loss, None
    
    @classmethod
    def from_pretrained(
        cls: builtins.type[T],
        pretrained_name_or_path: str | Path,
        *,
        config: CustomDiffusionConfigWrapper | None = None,
        force_download: bool = False,
        resume_download: bool | None = None,
        proxies: dict | None = None,
        token: str | bool | None = None,
        cache_dir: str | Path | None = None,
        local_files_only: bool = False,
        revision: str | None = None,
        strict: bool = False,
        **kwargs,
    ) -> T:
        """
        The policy is set in evaluation mode by default using `policy.eval()` (dropout modules are
        deactivated). To train it, you should first set it back in training mode with `policy.train()`.
        """
        if config is None:
            config = CustomDiffusionConfigWrapper.from_pretrained(
                pretrained_name_or_path=pretrained_name_or_path,
                force_download=force_download,
                resume_download=resume_download,
                proxies=proxies,
                token=token,
                cache_dir=cache_dir,
                local_files_only=local_files_only,
                revision=revision,
                **kwargs,
            )
        model_id = str(pretrained_name_or_path)
        # print(config)
        instance = cls(config, **kwargs)
        if os.path.isdir(model_id):
            print("Loading weights from local directory")
            model_file = os.path.join(model_id, SAFETENSORS_SINGLE_FILE)
            policy = cls._load_as_safetensor(instance, model_file, config.device, strict)
        else:
            try:
                model_file = hf_hub_download(
                    repo_id=model_id,
                    filename=SAFETENSORS_SINGLE_FILE,
                    revision=revision,
                    cache_dir=cache_dir,
                    force_download=force_download,
                    proxies=proxies,
                    resume_download=resume_download,
                    token=token,
                    local_files_only=local_files_only,
                )
                policy = cls._load_as_safetensor(instance, model_file, config.device, strict)
            except HfHubHTTPError as e:
                raise FileNotFoundError(
                    f"{SAFETENSORS_SINGLE_FILE} not found on the HuggingFace Hub in {model_id}"
                ) from e

        policy.to(config.device)
        policy.eval()
        return policy